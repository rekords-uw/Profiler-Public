{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from profiler.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instantiate Engine\n",
    "* workers : number of processes\n",
    "* tol     : tolerance for differences when creating training data (set to 0 if data is completely clean)\n",
    "* eps     : error bound for inverse covariance estimation (since we use conservative calculation when determining minimum sample size, we recommend to set eps <= 0.01)\n",
    "* embedtxt: if set to true, differentiate b/w textual data and categorical data, and use word embedding for the former"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pf = Profiler(workers=2, tol=1e-6, eps=0.05, embedtxt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data\n",
    "* name: any name you like\n",
    "* src: \\[FILE; DF; DB (not implemented)\\]\n",
    "* fpath: required if src == FILE\n",
    "* df: required if src == DF\n",
    "* check_param: print parameters used for data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:profiler.utility:[0.010299205780029297] Load Data start\n",
      "\n",
      "INFO:profiler.data.dataset:parameters used for data loading:\n",
      " {'na_values': 'empty', 'sep': ',', 'header': 'infer', 'dropcol': None, 'dropna': False, 'encoding': 'utf-8', 'normalize': True, 'min_categories_for_embedding': 10}\n",
      "WARNING:profiler.data.dataset:Dropping the following null column from the dataset: 'Address2'\n",
      "WARNING:profiler.data.dataset:Dropping the following null column from the dataset: 'Address3'\n",
      "INFO:profiler.data.dataset:inferred types of attributes: {\n",
      "    \"ProviderNumber\": \"numeric\",\n",
      "    \"HospitalName\": \"text\",\n",
      "    \"Address1\": \"text\",\n",
      "    \"City\": \"text\",\n",
      "    \"State\": \"categorical\",\n",
      "    \"ZipCode\": \"numeric\",\n",
      "    \"CountyName\": \"text\",\n",
      "    \"PhoneNumber\": \"numeric\",\n",
      "    \"HospitalType\": \"categorical\",\n",
      "    \"HospitalOwner\": \"categorical\",\n",
      "    \"EmergencyService\": \"categorical\",\n",
      "    \"Condition\": \"categorical\",\n",
      "    \"MeasureCode\": \"text\",\n",
      "    \"MeasureName\": \"text\",\n",
      "    \"Score\": \"text\",\n",
      "    \"Sample\": \"text\",\n",
      "    \"Stateavg\": \"text\"\n",
      "}\n",
      "INFO:profiler.data.dataset:(possible types: numeric, categorical, text, date)\n",
      "INFO:profiler.data.dataset:inferred operators of attributes: {'ProviderNumber': ['equal'], 'HospitalName': ['equal'], 'Address1': ['equal'], 'City': ['equal'], 'State': ['equal'], 'ZipCode': ['equal'], 'CountyName': ['equal'], 'PhoneNumber': ['equal'], 'HospitalType': ['equal'], 'HospitalOwner': ['equal'], 'EmergencyService': ['equal'], 'Condition': ['equal'], 'MeasureCode': ['equal'], 'MeasureName': ['equal'], 'Score': ['equal'], 'Sample': ['equal'], 'Stateavg': ['equal']}\n",
      "INFO:profiler.data.dataset:(possible operators: equal, notequal, greater_than, less_than)\n",
      "INFO:profiler.utility:[0.06873345375061035] Load Data execution time: 0.058434247970581055\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pf.session.load_data(name='hospital', src=FILE, fpath='data/hospital_clean_unflatten.csv', check_param=True, na_values='empty')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Change Data Types of Attributes\n",
    "* required input:\n",
    "    * a list of attributes\n",
    "    * a list of data types (must match the order of the attributes; can be CATEGORICAL, NUMERIC, TEXT, DATE)\n",
    "* optional input:\n",
    "    * a list of regular expression extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pf.session.change_dtypes(['ProviderNumber', 'ZipCode', 'PhoneNumber', 'State', 'EmergencyService','Score', 'Sample','HospitalType','HospitalOwner', 'Condition'], \n",
    "#                             [CATEGORICAL, NUMERIC, CATEGORICAL, TEXT, TEXT, NUMERIC, NUMERIC, TEXT,TEXT, TEXT],\n",
    "#                             [None, None, None, None, None, r'(\\d+)%', r'(\\d+)\\spatients', None, None,None])\n",
    "# # pf.session.change_dtypes(['ProviderNumber', 'ZipCode', 'PhoneNumber', 'State', 'EmergencyService','Score', 'Sample'], \n",
    "# #                             [CATEGORICAL, CATEGORICAL, CATEGORICAL, TEXT, TEXT, NUMERIC, NUMERIC],\n",
    "# #                             [None, None, None, None, None, r'(\\d+)%', r'(\\d+)\\spatients'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Load/Train Embeddings for TEXT\n",
    "* path: path to saved/to-save embedding folder\n",
    "* load: set to true -- load saved vec from 'path'; set to false -- train locally\n",
    "* save: (only for load = False) save trained vectors to 'path'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:profiler.utility:[0.08297181129455566] Load Embedding start\n",
      "\n",
      "INFO:profiler.data.embedding:[HospitalName] tokenize cell\n",
      "INFO:profiler.data.embedding:[Address1] tokenize cell\n",
      "INFO:profiler.data.embedding:[HospitalName] train language model\n",
      "INFO:profiler.data.embedding:[Address1] train language model\n",
      "INFO:profiler.data.embedding:[HospitalName] compute weights\n",
      "INFO:profiler.data.embedding:[HospitalName] create vector map\n",
      "INFO:profiler.data.embedding:[HospitalName] save vec and vocab\n",
      "INFO:profiler.data.embedding:[City] tokenize cell\n",
      "INFO:profiler.data.embedding:[City] train language model\n",
      "INFO:profiler.data.embedding:[Address1] compute weights\n",
      "INFO:profiler.data.embedding:[Address1] create vector map\n",
      "INFO:profiler.data.embedding:[Address1] save vec and vocab\n",
      "INFO:profiler.data.embedding:[CountyName] tokenize cell\n",
      "INFO:profiler.data.embedding:[CountyName] train language model\n",
      "INFO:profiler.data.embedding:[City] compute weights\n",
      "INFO:profiler.data.embedding:[City] create vector map\n",
      "INFO:profiler.data.embedding:[City] save vec and vocab\n",
      "INFO:profiler.data.embedding:[MeasureCode] tokenize cell\n",
      "INFO:profiler.data.embedding:[MeasureCode] train language model\n",
      "INFO:profiler.data.embedding:[CountyName] compute weights\n",
      "INFO:profiler.data.embedding:[CountyName] create vector map\n",
      "INFO:profiler.data.embedding:[CountyName] save vec and vocab\n",
      "INFO:profiler.data.embedding:[MeasureName] tokenize cell\n",
      "INFO:profiler.data.embedding:[MeasureName] train language model\n",
      "INFO:profiler.data.embedding:[MeasureCode] compute weights\n",
      "INFO:profiler.data.embedding:[MeasureCode] create vector map\n",
      "INFO:profiler.data.embedding:[MeasureCode] save vec and vocab\n",
      "INFO:profiler.data.embedding:[Score] tokenize cell\n",
      "INFO:profiler.data.embedding:[Score] train language model\n",
      "INFO:profiler.data.embedding:[MeasureName] compute weights\n",
      "INFO:profiler.data.embedding:[MeasureName] create vector map\n",
      "INFO:profiler.data.embedding:[MeasureName] save vec and vocab\n",
      "INFO:profiler.data.embedding:[Sample] tokenize cell\n",
      "INFO:profiler.data.embedding:[Sample] train language model\n",
      "INFO:profiler.data.embedding:[Score] compute weights\n",
      "INFO:profiler.data.embedding:[Score] create vector map\n",
      "INFO:profiler.data.embedding:[Score] save vec and vocab\n",
      "INFO:profiler.data.embedding:[Stateavg] tokenize cell\n",
      "INFO:profiler.data.embedding:[Stateavg] train language model\n",
      "INFO:profiler.data.embedding:[Sample] compute weights\n",
      "INFO:profiler.data.embedding:[Sample] create vector map\n",
      "INFO:profiler.data.embedding:[Sample] save vec and vocab\n",
      "INFO:profiler.data.embedding:[Stateavg] compute weights\n",
      "INFO:profiler.data.embedding:[Stateavg] create vector map\n",
      "INFO:profiler.data.embedding:[Stateavg] save vec and vocab\n",
      "INFO:profiler.utility:[179.44697642326355] Load Embedding execution time: 179.364004611969\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pf.session.load_embedding(save=True, path='data/hospital/', load=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Training Data\n",
    "* multiplier: if set to None, will infer the minimal sample size; otherwise, it will create (# samples) * (# attributes) * (multiplier) training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:profiler.utility:[179.45400309562683] Create Training Data start\n",
      "\n",
      "INFO:profiler.data.transformer:needs multiplier = 10 to bound the error in inv cov estimation <= 0.05000000\n",
      "INFO:profiler.data.transformer:use multiplier = 10, and the bound is 0.00120416\n",
      "INFO:profiler.data.transformer:Draw Pairs\n",
      "100%|██████████| 17/17 [00:00<00:00, 249.71it/s]\n",
      "INFO:profiler.data.transformer:Number of training samples: 17000\n",
      "INFO:profiler.data.transformer:Computing Differences\n",
      "INFO:profiler.data.transformer:estimated missing data probability in training data is 0.0000\n",
      "INFO:profiler.utility:[186.10093212127686] Create Training Data execution time: 6.646929025650024\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use simple empirical cov: difference=False\n",
    "# use difference -> cov : difference=True\n",
    "pf.session.load_training_data(multiplier = None, difference=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Learn Structure\n",
    "* sparsity: intensity of L1-regularizer in inverse covariance estimation (glasso)\n",
    "* take_neg: if set to true, consider equal -> equal only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:profiler.utility:[186.10677289962769] Learn Structure start\n",
      "\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-5dc0449672fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# set sparsity to 0 for exp_reproduce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mautoregress_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparsity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfer_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/c/Users/Yunjia/Onedrive - UW-Madison/Database/Profiler/Profiler-Public/profiler/core.py\u001b[0m in \u001b[0;36mlearn_structure\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlearn_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Learn Structure'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstruct_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Learn Structure'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/Yunjia/Onedrive - UW-Madison/Database/Profiler/Profiler-Public/profiler/learner.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcov\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate_covariance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv_cov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate_inverse_covariance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcov\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;31m# self.visualize_inverse_covariance()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meigvals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv_cov\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/Yunjia/Onedrive - UW-Madison/Database/Profiler/Profiler-Public/profiler/learner.py\u001b[0m in \u001b[0;36mestimate_inverse_covariance\u001b[0;34m(self, cov)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrans_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         est_cov, inv_cov = graphical_lasso(cov, alpha=self.param['sparsity'], mode=self.param['solver'],\n\u001b[0;32m--> 279\u001b[0;31m                                            max_iter=self.param['max_iter'])\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minv_cov\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;31m# apply threshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda3/lib/python3.6/site-packages/sklearn/covariance/graph_lasso_.py\u001b[0m in \u001b[0;36mgraphical_lasso\u001b[0;34m(emp_cov, alpha, cov_init, mode, tol, enet_tol, max_iter, verbose, return_costs, eps, return_n_iter)\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0memp_cov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memp_cov\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0memp_cov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memp_cov\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcov_init\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0mcovariance_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memp_cov\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda3/lib/python3.6/site-packages/scipy/linalg/basic.py\u001b[0m in \u001b[0;36minv\u001b[0;34m(a, overwrite_a, check_finite)\u001b[0m\n\u001b[1;32m    972\u001b[0m         \u001b[0minv_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpiv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlwork\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite_lu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 974\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"singular matrix\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    975\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m         raise ValueError('illegal value in %d-th argument of internal '\n",
      "\u001b[0;31mLinAlgError\u001b[0m: singular matrix"
     ]
    }
   ],
   "source": [
    "# set sparsity to 0 for exp_reproduce \n",
    "autoregress_matrix = pf.session.learn_structure(sparsity=0, infer_order=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* score: \n",
    "    * \"fit_error\": mse for fitting y = B'X + c for each atttribute y \n",
    "    * \"training_data_fd_vio_ratio\": the higher the score, the more violations of FDs in the training data. (bounded: \\[0,1\\])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parent_sets = pf.session.get_dependencies(score=\"fit_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, json\n",
    "def read_fds(path='data/fds', f='TECHospital-hyfd'):\n",
    "    all_fds = {}\n",
    "    for line in open(os.path.join(path,f)):\n",
    "        fd = json.loads(line)\n",
    "        right = fd[u'dependant']['columnIdentifier']\n",
    "        left = [l[u'columnIdentifier'] for l in fd[ u'determinant'][u'columnIdentifiers']]\n",
    "        if right not in all_fds:\n",
    "            all_fds[right] = set()\n",
    "        all_fds[right].add(frozenset(left))\n",
    "    return all_fds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gt = read_fds(f='hospital_clean-fun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tp = 0\n",
    "count = 0\n",
    "for child in parent_sets:\n",
    "    found = parent_sets[child]\n",
    "    if len(found) == 0:\n",
    "        continue\n",
    "    count += 1\n",
    "    match = False\n",
    "    for parent in gt[child]:\n",
    "        if set(parent).issubset(found):\n",
    "            tp += 1\n",
    "            match = True\n",
    "            break\n",
    "    if not match:\n",
    "        print(\"{} -> {} is not valid\".format(found, child))\n",
    "if count > 0:\n",
    "    print(\"Precision: %.4f\"%(float(tp) / count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pf.session.visualize_covariance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pf.session.visualize_inverse_covariance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pf.session.visualize_autoregression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pf.session.timer.get_stat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pf.session.timer.to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
